{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "spark = SparkSession.builder.appName(\"dates\").getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read in data and store in dataframe\n",
    "#csv = \"Resources/test10K.csv\"\n",
    "csv = \"Resources/GoBikeMerged2017_2018.csv\"\n",
    "df = spark.read.csv(csv, header=True, inferSchema=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-------+------------+--------------+--------------------+---------------------+--------------------+--------------------+-----------------+-------------+----------------+----------------------+-----------------------+--------------------+--------------------+----------+------------------+-------------+------------------+-----------+-------+--------+-------------------+\n",
      "|_c0|bike_id|duration_sec|end_station_id|end_station_latitude|end_station_longitude|    end_station_name|            end_time|member_birth_year|member_gender|start_station_id|start_station_latitude|start_station_longitude|  start_station_name|          start_time| user_type|start_neighborhood|start_zipcode|  end_neighborhood|end_zipcode|temp(f)|pressure|weather_description|\n",
      "+---+-------+------------+--------------+--------------------+---------------------+--------------------+--------------------+-----------------+-------------+----------------+----------------------+-----------------------+--------------------+--------------------+----------+------------------+-------------+------------------+-----------+-------+--------+-------------------+\n",
      "|  0|    240|         424|          48.0|  37.782411189735896|  -122.39270595839115| 2nd St at S Park St|2017-06-28 09:54:...|           1985.0|       Female|            21.0|            37.7896254|            -122.400811|Montgomery St BAR...|2017-06-28 09:47:...|Subscriber|   South of Market|      94104.0|       China Basin|    94107.0|   62.0|  1021.0|         light_rain|\n",
      "|  1|    669|         366|          59.0|           37.774814|          -122.418954|S Van Ness Ave at...|2017-06-28 09:53:...|           1981.0|         Male|            58.0|             37.776619|            -122.417385|Market St at 10th St|2017-06-28 09:47:...|Subscriber|      Civic Center|      94103.0|   South of Market|    94103.0|   62.0|  1021.0|         light_rain|\n",
      "|  2|    117|         188|          48.0|  37.782411189735896|  -122.39270595839115| 2nd St at S Park St|2017-06-28 09:52:...|           1984.0|         Male|            25.0|     37.78752178045625|    -122.39740490913393| Howard St at 2nd St|2017-06-28 09:49:...|Subscriber|      The East Cut|      94105.0|       China Basin|    94107.0|   62.0|  1021.0|         light_rain|\n",
      "|  3|     77|        1201|           9.0|   37.79857210846257|  -122.40086898207666|Broadway at Batte...|2017-06-28 10:11:...|           1985.0|         Male|            81.0|              37.77588|             -122.39317|  Berry St at 4th St|2017-06-28 09:50:...|Subscriber|       China Basin|      94107.0|Financial District|    94111.0|   62.0|  1021.0|         light_rain|\n",
      "|  4|    316|         431|         321.0|   37.78014570345598|  -122.40307085237872|       5th at Folsom|2017-06-28 10:03:...|           1973.0|         Male|            66.0|     37.77874161153677|    -122.39274082710837|3rd St at Townsen...|2017-06-28 09:56:...|Subscriber|       China Basin|      94107.0|   South of Market|    94107.0|   62.0|  1021.0|         light_rain|\n",
      "|  5|    605|        1086|          90.0|           37.771058|          -122.402717|Townsend St at 7t...|2017-06-28 10:15:...|           1958.0|         Male|            15.0|             37.795392|            -122.394203|San Francisco Fer...|2017-06-28 09:56:...|Subscriber|       Embarcadero|      94105.0|       Mission Bay|    94103.0|   62.0|  1021.0|         light_rain|\n",
      "|  6|    272|         730|          44.0|          37.7810737|         -122.4117382|Civic Center/UN P...|2017-06-28 10:10:...|           1980.0|         Male|            23.0|     37.79146400000001|            -122.391034|The Embarcadero a...|2017-06-28 09:58:...|Subscriber|       South Beach|         null|        Mid-Market|    94103.0|   62.0|  1021.0|         light_rain|\n",
      "|  7|    400|         435|          45.0|           37.781752|          -122.405127| 5th St at Howard St|2017-06-28 10:08:...|           1991.0|         Male|            81.0|              37.77588|             -122.39317|  Berry St at 4th St|2017-06-28 10:00:...|Subscriber|       China Basin|      94107.0|   South of Market|    94103.0|   64.0|  1015.0|         light_rain|\n",
      "|  8|    212|         590|          22.0|           37.789756|          -122.394643|Howard St at Beal...|2017-06-28 10:10:...|           1983.0|         Male|            66.0|     37.77874161153677|    -122.39274082710837|3rd St at Townsen...|2017-06-28 10:00:...|Subscriber|       China Basin|      94107.0|   South of Market|    94105.0|   64.0|  1015.0|         light_rain|\n",
      "|  9|    915|         553|          50.0|           37.780526|  -122.39028799999998|2nd St at Townsen...|2017-06-28 10:18:...|           1973.0|         Male|            15.0|             37.795392|            -122.394203|San Francisco Fer...|2017-06-28 10:09:...|Subscriber|       Embarcadero|      94105.0|   South of Market|    94107.0|   64.0|  1015.0|         light_rain|\n",
      "| 10|    331|         640|          21.0|          37.7896254|          -122.400811|Montgomery St BAR...|2017-06-28 10:22:...|           1978.0|         Male|            66.0|     37.77874161153677|    -122.39274082710837|3rd St at Townsen...|2017-06-28 10:11:...|Subscriber|       China Basin|      94107.0|   South of Market|    94104.0|   64.0|  1015.0|         light_rain|\n",
      "| 11|    216|        5441|          42.0|            37.77865|           -122.41823|San Francisco Cit...|2017-06-28 11:42:...|           1974.0|         Male|            45.0|             37.781752|            -122.405127| 5th St at Howard St|2017-06-28 10:11:...|  Customer|   South of Market|      94103.0|      Civic Center|    94102.0|   64.0|  1015.0|         light_rain|\n",
      "| 12|    106|        2431|          42.0|            37.77865|           -122.41823|San Francisco Cit...|2017-06-28 10:53:...|           1979.0|       Female|            15.0|             37.795392|            -122.394203|San Francisco Fer...|2017-06-28 10:12:...|Subscriber|       Embarcadero|      94105.0|      Civic Center|    94102.0|   64.0|  1015.0|         light_rain|\n",
      "| 13|    200|         889|          23.0|   37.79146400000001|          -122.391034|The Embarcadero a...|2017-06-28 10:27:...|           1984.0|       Female|            15.0|             37.795392|            -122.394203|San Francisco Fer...|2017-06-28 10:12:...|Subscriber|       Embarcadero|      94105.0|       South Beach|       null|   64.0|  1015.0|         light_rain|\n",
      "| 14|    122|        1909|          42.0|            37.77865|           -122.41823|San Francisco Cit...|2017-06-28 10:45:...|           1984.0|         Male|            15.0|             37.795392|            -122.394203|San Francisco Fer...|2017-06-28 10:13:...|Subscriber|       Embarcadero|      94105.0|      Civic Center|    94102.0|   64.0|  1015.0|         light_rain|\n",
      "| 15|    248|        1908|          42.0|            37.77865|           -122.41823|San Francisco Cit...|2017-06-28 10:45:...|           1989.0|         Male|            15.0|             37.795392|            -122.394203|San Francisco Fer...|2017-06-28 10:13:...|Subscriber|       Embarcadero|      94105.0|      Civic Center|    94102.0|   64.0|  1015.0|         light_rain|\n",
      "| 16|    196|         410|          15.0|           37.795392|          -122.394203|San Francisco Fer...|2017-06-28 10:21:...|           1979.0|         Male|            11.0|              37.79728|            -122.398436|Davis St at Jacks...|2017-06-28 10:14:...|Subscriber|Financial District|      94111.0|       Embarcadero|    94105.0|   64.0|  1015.0|         light_rain|\n",
      "| 17|     20|         278|          13.0|           37.794231|          -122.402923|Commercial St at ...|2017-06-28 10:20:...|           1952.0|         Male|            15.0|             37.795392|            -122.394203|San Francisco Fer...|2017-06-28 10:16:...|Subscriber|       Embarcadero|      94105.0|Financial District|    94111.0|   64.0|  1015.0|         light_rain|\n",
      "| 18|     21|        2227|          42.0|            37.77865|           -122.41823|San Francisco Cit...| 2017-06-28 10:53:38|           1945.0|         Male|            15.0|             37.795392|            -122.394203|San Francisco Fer...|2017-06-28 10:16:...|Subscriber|       Embarcadero|      94105.0|      Civic Center|    94102.0|   64.0|  1015.0|         light_rain|\n",
      "| 19|    118|        2349|          42.0|            37.77865|           -122.41823|San Francisco Cit...|2017-06-28 10:55:...|             null|         null|            28.0|     37.78716801474664|    -122.38809792330359|The Embarcadero a...|2017-06-28 10:16:...|  Customer|       Embarcadero|      94105.0|      Civic Center|    94102.0|   64.0|  1015.0|         light_rain|\n",
      "+---+-------+------------+--------------+--------------------+---------------------+--------------------+--------------------+-----------------+-------------+----------------+----------------------+-----------------------+--------------------+--------------------+----------+------------------+-------------+------------------+-----------+-------+--------+-------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Show dataframe\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+--------------------+--------+-------+-------------------+\n",
      "|_c0|          start_time|pressure|temp(f)|weather_description|\n",
      "+---+--------------------+--------+-------+-------------------+\n",
      "|  0|2017-06-28 09:47:...|  1021.0|   62.0|         light_rain|\n",
      "|  1|2017-06-28 09:47:...|  1021.0|   62.0|         light_rain|\n",
      "|  2|2017-06-28 09:49:...|  1021.0|   62.0|         light_rain|\n",
      "|  3|2017-06-28 09:50:...|  1021.0|   62.0|         light_rain|\n",
      "|  4|2017-06-28 09:56:...|  1021.0|   62.0|         light_rain|\n",
      "|  5|2017-06-28 09:56:...|  1021.0|   62.0|         light_rain|\n",
      "|  6|2017-06-28 09:58:...|  1021.0|   62.0|         light_rain|\n",
      "|  7|2017-06-28 10:00:...|  1015.0|   64.0|         light_rain|\n",
      "|  8|2017-06-28 10:00:...|  1015.0|   64.0|         light_rain|\n",
      "|  9|2017-06-28 10:09:...|  1015.0|   64.0|         light_rain|\n",
      "| 10|2017-06-28 10:11:...|  1015.0|   64.0|         light_rain|\n",
      "| 11|2017-06-28 10:11:...|  1015.0|   64.0|         light_rain|\n",
      "| 12|2017-06-28 10:12:...|  1015.0|   64.0|         light_rain|\n",
      "| 13|2017-06-28 10:12:...|  1015.0|   64.0|         light_rain|\n",
      "| 14|2017-06-28 10:13:...|  1015.0|   64.0|         light_rain|\n",
      "| 15|2017-06-28 10:13:...|  1015.0|   64.0|         light_rain|\n",
      "| 16|2017-06-28 10:14:...|  1015.0|   64.0|         light_rain|\n",
      "| 17|2017-06-28 10:16:...|  1015.0|   64.0|         light_rain|\n",
      "| 18|2017-06-28 10:16:...|  1015.0|   64.0|         light_rain|\n",
      "| 19|2017-06-28 10:16:...|  1015.0|   64.0|         light_rain|\n",
      "+---+--------------------+--------+-------+-------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df = df.select(['_c0', 'start_time', 'pressure', 'temp(f)', 'weather_description'])\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find day of the week\n",
    "from pyspark.sql.functions import dayofweek"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [],
   "source": [
    " #df.select(dayofweek('datetime_PST').alias('day')).collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+--------------------+--------+-------+-------------------+-----------+\n",
      "|_c0|          start_time|pressure|temp(f)|weather_description|day_of_week|\n",
      "+---+--------------------+--------+-------+-------------------+-----------+\n",
      "|  0|2017-06-28 09:47:...|  1021.0|   62.0|         light_rain|          4|\n",
      "|  1|2017-06-28 09:47:...|  1021.0|   62.0|         light_rain|          4|\n",
      "|  2|2017-06-28 09:49:...|  1021.0|   62.0|         light_rain|          4|\n",
      "|  3|2017-06-28 09:50:...|  1021.0|   62.0|         light_rain|          4|\n",
      "|  4|2017-06-28 09:56:...|  1021.0|   62.0|         light_rain|          4|\n",
      "|  5|2017-06-28 09:56:...|  1021.0|   62.0|         light_rain|          4|\n",
      "|  6|2017-06-28 09:58:...|  1021.0|   62.0|         light_rain|          4|\n",
      "|  7|2017-06-28 10:00:...|  1015.0|   64.0|         light_rain|          4|\n",
      "|  8|2017-06-28 10:00:...|  1015.0|   64.0|         light_rain|          4|\n",
      "|  9|2017-06-28 10:09:...|  1015.0|   64.0|         light_rain|          4|\n",
      "| 10|2017-06-28 10:11:...|  1015.0|   64.0|         light_rain|          4|\n",
      "| 11|2017-06-28 10:11:...|  1015.0|   64.0|         light_rain|          4|\n",
      "| 12|2017-06-28 10:12:...|  1015.0|   64.0|         light_rain|          4|\n",
      "| 13|2017-06-28 10:12:...|  1015.0|   64.0|         light_rain|          4|\n",
      "| 14|2017-06-28 10:13:...|  1015.0|   64.0|         light_rain|          4|\n",
      "| 15|2017-06-28 10:13:...|  1015.0|   64.0|         light_rain|          4|\n",
      "| 16|2017-06-28 10:14:...|  1015.0|   64.0|         light_rain|          4|\n",
      "| 17|2017-06-28 10:16:...|  1015.0|   64.0|         light_rain|          4|\n",
      "| 18|2017-06-28 10:16:...|  1015.0|   64.0|         light_rain|          4|\n",
      "| 19|2017-06-28 10:16:...|  1015.0|   64.0|         light_rain|          4|\n",
      "+---+--------------------+--------+-------+-------------------+-----------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Save the day of the week as a new column\n",
    "df = df.withColumn(\"day_of_week\", dayofweek('start_time'))\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+--------------------+--------+-------+-------------------+-----------+----+----+-----+------------+\n",
      "|_c0|          start_time|pressure|temp(f)|weather_description|day_of_week|hour|year|month|day_of_month|\n",
      "+---+--------------------+--------+-------+-------------------+-----------+----+----+-----+------------+\n",
      "|  0|2017-06-28 09:47:...|  1021.0|   62.0|         light_rain|          4|   9|2017|    6|          28|\n",
      "|  1|2017-06-28 09:47:...|  1021.0|   62.0|         light_rain|          4|   9|2017|    6|          28|\n",
      "|  2|2017-06-28 09:49:...|  1021.0|   62.0|         light_rain|          4|   9|2017|    6|          28|\n",
      "|  3|2017-06-28 09:50:...|  1021.0|   62.0|         light_rain|          4|   9|2017|    6|          28|\n",
      "|  4|2017-06-28 09:56:...|  1021.0|   62.0|         light_rain|          4|   9|2017|    6|          28|\n",
      "|  5|2017-06-28 09:56:...|  1021.0|   62.0|         light_rain|          4|   9|2017|    6|          28|\n",
      "|  6|2017-06-28 09:58:...|  1021.0|   62.0|         light_rain|          4|   9|2017|    6|          28|\n",
      "|  7|2017-06-28 10:00:...|  1015.0|   64.0|         light_rain|          4|  10|2017|    6|          28|\n",
      "|  8|2017-06-28 10:00:...|  1015.0|   64.0|         light_rain|          4|  10|2017|    6|          28|\n",
      "|  9|2017-06-28 10:09:...|  1015.0|   64.0|         light_rain|          4|  10|2017|    6|          28|\n",
      "| 10|2017-06-28 10:11:...|  1015.0|   64.0|         light_rain|          4|  10|2017|    6|          28|\n",
      "| 11|2017-06-28 10:11:...|  1015.0|   64.0|         light_rain|          4|  10|2017|    6|          28|\n",
      "| 12|2017-06-28 10:12:...|  1015.0|   64.0|         light_rain|          4|  10|2017|    6|          28|\n",
      "| 13|2017-06-28 10:12:...|  1015.0|   64.0|         light_rain|          4|  10|2017|    6|          28|\n",
      "| 14|2017-06-28 10:13:...|  1015.0|   64.0|         light_rain|          4|  10|2017|    6|          28|\n",
      "| 15|2017-06-28 10:13:...|  1015.0|   64.0|         light_rain|          4|  10|2017|    6|          28|\n",
      "| 16|2017-06-28 10:14:...|  1015.0|   64.0|         light_rain|          4|  10|2017|    6|          28|\n",
      "| 17|2017-06-28 10:16:...|  1015.0|   64.0|         light_rain|          4|  10|2017|    6|          28|\n",
      "| 18|2017-06-28 10:16:...|  1015.0|   64.0|         light_rain|          4|  10|2017|    6|          28|\n",
      "| 19|2017-06-28 10:16:...|  1015.0|   64.0|         light_rain|          4|  10|2017|    6|          28|\n",
      "+---+--------------------+--------+-------+-------------------+-----------+----+----+-----+------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Save the day of the week as a new column\n",
    "from pyspark.sql.functions import year, month, dayofmonth\n",
    "df = df.withColumn(\"year\", year('start_time'))\n",
    "df = df.withColumn(\"month\", month('start_time'))\n",
    "df = df.withColumn(\"day_of_month\", dayofmonth('start_time'))\n",
    "\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filtering out weekend days (day 1 is Sunday, day 7 is Saturday)\n",
    "df_weekdays = df.filter( (df[\"day_of_week\"] > 1) | (df['day_of_week'] < 7) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+--------------------+--------+-------+-------------------+-----------+----+----+-----+------------+\n",
      "|_c0|          start_time|pressure|temp(f)|weather_description|day_of_week|hour|year|month|day_of_month|\n",
      "+---+--------------------+--------+-------+-------------------+-----------+----+----+-----+------------+\n",
      "|  0|2017-06-28 09:47:...|  1021.0|   62.0|         light_rain|          4|   9|2017|    6|          28|\n",
      "|  1|2017-06-28 09:47:...|  1021.0|   62.0|         light_rain|          4|   9|2017|    6|          28|\n",
      "|  2|2017-06-28 09:49:...|  1021.0|   62.0|         light_rain|          4|   9|2017|    6|          28|\n",
      "|  3|2017-06-28 09:50:...|  1021.0|   62.0|         light_rain|          4|   9|2017|    6|          28|\n",
      "|  4|2017-06-28 09:56:...|  1021.0|   62.0|         light_rain|          4|   9|2017|    6|          28|\n",
      "|  5|2017-06-28 09:56:...|  1021.0|   62.0|         light_rain|          4|   9|2017|    6|          28|\n",
      "|  6|2017-06-28 09:58:...|  1021.0|   62.0|         light_rain|          4|   9|2017|    6|          28|\n",
      "|  7|2017-06-28 10:00:...|  1015.0|   64.0|         light_rain|          4|  10|2017|    6|          28|\n",
      "|  8|2017-06-28 10:00:...|  1015.0|   64.0|         light_rain|          4|  10|2017|    6|          28|\n",
      "|  9|2017-06-28 10:09:...|  1015.0|   64.0|         light_rain|          4|  10|2017|    6|          28|\n",
      "| 10|2017-06-28 10:11:...|  1015.0|   64.0|         light_rain|          4|  10|2017|    6|          28|\n",
      "| 11|2017-06-28 10:11:...|  1015.0|   64.0|         light_rain|          4|  10|2017|    6|          28|\n",
      "| 12|2017-06-28 10:12:...|  1015.0|   64.0|         light_rain|          4|  10|2017|    6|          28|\n",
      "| 13|2017-06-28 10:12:...|  1015.0|   64.0|         light_rain|          4|  10|2017|    6|          28|\n",
      "| 14|2017-06-28 10:13:...|  1015.0|   64.0|         light_rain|          4|  10|2017|    6|          28|\n",
      "| 15|2017-06-28 10:13:...|  1015.0|   64.0|         light_rain|          4|  10|2017|    6|          28|\n",
      "| 16|2017-06-28 10:14:...|  1015.0|   64.0|         light_rain|          4|  10|2017|    6|          28|\n",
      "| 17|2017-06-28 10:16:...|  1015.0|   64.0|         light_rain|          4|  10|2017|    6|          28|\n",
      "| 18|2017-06-28 10:16:...|  1015.0|   64.0|         light_rain|          4|  10|2017|    6|          28|\n",
      "| 19|2017-06-28 10:16:...|  1015.0|   64.0|         light_rain|          4|  10|2017|    6|          28|\n",
      "+---+--------------------+--------+-------+-------------------+-----------+----+----+-----+------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_weekdays.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [],
   "source": [
    "#To do\n",
    "#Filter out holidays"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+--------------------+--------+-------+-------------------+-----------+----+\n",
      "|_c0|          start_time|pressure|temp(f)|weather_description|day_of_week|hour|\n",
      "+---+--------------------+--------+-------+-------------------+-----------+----+\n",
      "|  0|2017-06-28 09:47:...|  1021.0|   62.0|         light_rain|          4|   9|\n",
      "|  1|2017-06-28 09:47:...|  1021.0|   62.0|         light_rain|          4|   9|\n",
      "|  2|2017-06-28 09:49:...|  1021.0|   62.0|         light_rain|          4|   9|\n",
      "|  3|2017-06-28 09:50:...|  1021.0|   62.0|         light_rain|          4|   9|\n",
      "|  4|2017-06-28 09:56:...|  1021.0|   62.0|         light_rain|          4|   9|\n",
      "|  5|2017-06-28 09:56:...|  1021.0|   62.0|         light_rain|          4|   9|\n",
      "|  6|2017-06-28 09:58:...|  1021.0|   62.0|         light_rain|          4|   9|\n",
      "|  7|2017-06-28 10:00:...|  1015.0|   64.0|         light_rain|          4|  10|\n",
      "|  8|2017-06-28 10:00:...|  1015.0|   64.0|         light_rain|          4|  10|\n",
      "|  9|2017-06-28 10:09:...|  1015.0|   64.0|         light_rain|          4|  10|\n",
      "| 10|2017-06-28 10:11:...|  1015.0|   64.0|         light_rain|          4|  10|\n",
      "| 11|2017-06-28 10:11:...|  1015.0|   64.0|         light_rain|          4|  10|\n",
      "| 12|2017-06-28 10:12:...|  1015.0|   64.0|         light_rain|          4|  10|\n",
      "| 13|2017-06-28 10:12:...|  1015.0|   64.0|         light_rain|          4|  10|\n",
      "| 14|2017-06-28 10:13:...|  1015.0|   64.0|         light_rain|          4|  10|\n",
      "| 15|2017-06-28 10:13:...|  1015.0|   64.0|         light_rain|          4|  10|\n",
      "| 16|2017-06-28 10:14:...|  1015.0|   64.0|         light_rain|          4|  10|\n",
      "| 17|2017-06-28 10:16:...|  1015.0|   64.0|         light_rain|          4|  10|\n",
      "| 18|2017-06-28 10:16:...|  1015.0|   64.0|         light_rain|          4|  10|\n",
      "| 19|2017-06-28 10:16:...|  1015.0|   64.0|         light_rain|          4|  10|\n",
      "+---+--------------------+--------+-------+-------------------+-----------+----+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Save the hour of the day as a new column\n",
    "from pyspark.sql.functions import hour\n",
    "df = df.withColumn(\"hour\", hour('start_time'))\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+-----+------------+----+-----------+--------+-------+-------------------+-----+\n",
      "|year|month|day_of_month|hour|day_of_week|pressure|temp(f)|weather_description|count|\n",
      "+----+-----+------------+----+-----------+--------+-------+-------------------+-----+\n",
      "|2017|    7|           1|   2|          7|  1017.0|   61.0|         light_rain|    3|\n",
      "|2017|    7|           6|   9|          5|  1019.0|   66.0|       thunderstorm|  113|\n",
      "|2017|    7|          29|   2|          7|  1014.0|   59.0|         light_rain|    3|\n",
      "|2017|    7|          31|   2|          2|  1018.0|   60.0|         light_rain|    1|\n",
      "|2017|    8|           9|  13|          4|  1016.0|   68.0|             cloudy|   96|\n",
      "|2017|    8|          20|  18|          1|  1011.0|   76.0|        clear_skies|   95|\n",
      "|2017|    8|          31|  15|          5|  1009.0|   89.0|              foggy|  137|\n",
      "|2017|    9|           6|   5|          4|  1014.0|   66.0|              foggy|   17|\n",
      "|2017|    9|           8|  15|          6|  1012.0|   77.0|        clear_skies|  184|\n",
      "|2017|    9|          17|  13|          1|  1016.0|   72.0|             cloudy|  215|\n",
      "|2017|    9|          21|  11|          5|  1013.0|   65.0|             cloudy|  160|\n",
      "|2017|    9|          27|  14|          4|  1012.0|   86.0|        clear_skies|  109|\n",
      "|2017|   10|           5|  16|          5|  1015.0|   81.0|        clear_skies|  268|\n",
      "|2017|   10|           7|  21|          7|  1009.0|   64.0|        clear_skies|   52|\n",
      "|2017|   10|          16|  21|          2|  1017.0|   67.0|              foggy|  172|\n",
      "|2017|   10|          28|  21|          7|  1013.0|   59.0|         light_rain|   36|\n",
      "|2017|   11|           4|  22|          7|  1018.0|   53.0|             cloudy|   34|\n",
      "|2017|   11|           8|   5|          4|  1016.0|   50.0|             cloudy|   16|\n",
      "|2017|   11|           8|  19|          4|  1012.0|   64.0|         light_rain|  278|\n",
      "|2017|   11|          10|  15|          6|  1017.0|   65.0|         heavy_rain|  171|\n",
      "+----+-----+------------+----+-----------+--------+-------+-------------------+-----+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "grouped_df = df.groupBy(['year', 'month', 'day_of_month', 'hour', 'day_of_week', 'pressure', 'temp(f)', 'weather_description']).count()\n",
    "#grouped_df = df.groupBy(['hour']).count()\n",
    "grouped_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [],
   "source": [
    "#This is a simple data set with day of the week, hour and bikeride count\n",
    "#simple_df = df.groupBy(['hour', 'day_of_week']).count()\n",
    "#simple_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import sklearn\n",
    "import sklearn.datasets\n",
    "from numpy.random import seed\n",
    "seed(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "pandas_df = grouped_df.toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Removing rows with no hours\n",
    "pandas_df = pandas_df[pandas_df.hour.notnull()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(9606, 9)"
      ]
     },
     "execution_count": 158,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Count number of rows\n",
    "pandas_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(9606, 2) (9606,)\n"
     ]
    }
   ],
   "source": [
    "#X = pandas_df.drop(\"count\", axis=1)\n",
    "X = pandas_df.drop(columns=['year', 'month', 'day_of_month', 'pressure', 'temp(f)', 'weather_description', 'count'], axis=1)\n",
    "y = pandas_df[\"count\"]\n",
    "print(X.shape, y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "metadata": {},
   "outputs": [],
   "source": [
    "#X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "metadata": {},
   "outputs": [],
   "source": [
    "#np.unique(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder, StandardScaler\n",
    "from keras.utils import to_categorical\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, random_state=1, stratify=None)\n",
    "\n",
    "#Using stratify=y failed, throwing error. What does it actually do?\n",
    "#X_train, X_test, y_train, y_test = train_test_split(\n",
    "#    X, y, random_state=1, stratify=y)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_scaler = StandardScaler().fit(X_train)\n",
    "X_train_scaled = X_scaler.transform(X_train)\n",
    "X_test_scaled = X_scaler.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       ...,\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.]], dtype=float32)"
      ]
     },
     "execution_count": 197,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# No need to label-encode y (because it's made of numbers)\n",
    "#label_encoder = LabelEncoder()\n",
    "#y_scaler = StandardScaler().fit(y_train)\n",
    "#y_train_scaled = y_scaler.transform(y_train)\n",
    "#y_test_scaled = y_scaler.transform(y_test)\n",
    "\n",
    "#Do I need one-hot encoding even if the labels were not strings?\n",
    "\n",
    "# One-hot encoding\n",
    "y_train_categorical = to_categorical(y_train)\n",
    "y_test_categorical = to_categorical(y_test)\n",
    "y_train_categorical"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "metadata": {},
   "outputs": [],
   "source": [
    "# first, create a normal neural network with 2 inputs, 6 hidden nodes, and 2 outputs\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Dense(units=6, activation='relu', input_dim=3))\n",
    "model.add(Dense(units=2, activation='softmax'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compile the model\n",
    "model.compile(optimizer='adam',\n",
    "              loss='categorical_crossentropy',\n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Error when checking input: expected dense_5_input to have shape (3,) but got array with shape (2,)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-203-48b0713203d2>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m100\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m     \u001b[0mshuffle\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m     \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m )\n",
      "\u001b[0;32m~/anaconda3/envs/MachineLearning/lib/python3.6/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, **kwargs)\u001b[0m\n\u001b[1;32m    953\u001b[0m             \u001b[0msample_weight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msample_weight\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    954\u001b[0m             \u001b[0mclass_weight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mclass_weight\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 955\u001b[0;31m             batch_size=batch_size)\n\u001b[0m\u001b[1;32m    956\u001b[0m         \u001b[0;31m# Prepare validation data.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    957\u001b[0m         \u001b[0mdo_validation\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/MachineLearning/lib/python3.6/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36m_standardize_user_data\u001b[0;34m(self, x, y, sample_weight, class_weight, check_array_lengths, batch_size)\u001b[0m\n\u001b[1;32m    752\u001b[0m             \u001b[0mfeed_input_shapes\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    753\u001b[0m             \u001b[0mcheck_batch_axis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m  \u001b[0;31m# Don't enforce the batch size.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 754\u001b[0;31m             exception_prefix='input')\n\u001b[0m\u001b[1;32m    755\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    756\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0my\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/MachineLearning/lib/python3.6/site-packages/keras/engine/training_utils.py\u001b[0m in \u001b[0;36mstandardize_input_data\u001b[0;34m(data, names, shapes, check_batch_axis, exception_prefix)\u001b[0m\n\u001b[1;32m    134\u001b[0m                             \u001b[0;34m': expected '\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mnames\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m' to have shape '\u001b[0m \u001b[0;34m+\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    135\u001b[0m                             \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m' but got array with shape '\u001b[0m \u001b[0;34m+\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 136\u001b[0;31m                             str(data_shape))\n\u001b[0m\u001b[1;32m    137\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    138\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: Error when checking input: expected dense_5_input to have shape (3,) but got array with shape (2,)"
     ]
    }
   ],
   "source": [
    "# Fit the model to the training data\n",
    "model.fit(\n",
    "    X_train_scaled,\n",
    "    y_train_categorical,\n",
    "    epochs=100,\n",
    "    shuffle=True,\n",
    "    verbose=2\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
